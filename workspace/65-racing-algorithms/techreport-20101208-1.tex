% Clever Algorithms: Benchmarking Algorithms

% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

\documentclass[a4paper, 11pt]{article}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{url}
\usepackage[pdftex,breaklinks=true,colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue,]{hyperref}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=25mm,bmargin=25mm,lmargin=25mm,rmargin=25mm}

% Dear template user: fill these in
\newcommand{\myreporttitle}{Clever Algorithms}
\newcommand{\myreportsubtitle}{Benchmarking Algorithms}
\newcommand{\myreportauthor}{Jason Brownlee}
\newcommand{\myreportemail}{jasonb@CleverAlgorithms.com}
\newcommand{\myreportproject}{The Clever Algorithms Project\\\url{http://www.CleverAlgorithms.com}}
\newcommand{\myreportdate}{20101208}
\newcommand{\myreportfulldate}{December 08, 2010}
\newcommand{\myreportversion}{1}
\newcommand{\myreportlicense}{\copyright\ Copyright 2010 Jason Brownlee. Some Rights Reserved. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.}

% leave this alone, it's templated baby!
\title{{\myreporttitle}: {\myreportsubtitle}\footnote{\myreportlicense}}
\author{\myreportauthor\\{\myreportemail}\\\small\myreportproject}
\date{\myreportfulldate\\{\small{Technical Report: CA-TR-{\myreportdate}-\myreportversion}}}
\begin{document}
\maketitle

% write a summary sentence for each major section
\section*{Abstract} 
% project
The Clever Algorithms project aims to describe a large number of Artificial Intelligence algorithms in a complete, consistent, and centralized manner, to improve their general accessibility. 
% template
The project makes use of a standardized algorithm description template that uses well-defined topics that motivate the collection of specific and useful information about each algorithm described.
% report
This report...

\begin{description}
	\item[Keywords:] {\small\texttt{Clever, Algorithms, Racing, Benchmarking}}
\end{description} 

% summarise the document breakdown with cross references
\section{Introduction}
\label{sec:introduction}
% project
The Clever Algorithms project aims to describe a large number of algorithms from the fields of Computational Intelligence, Biologically Inspired Computation, and Metaheuristics in a complete, consistent and centralized manner \cite{Brownlee2010}.
% description
The project requires all algorithms to be described using a standardized template that includes a fixed number of sections, each of which is motivated by the presentation of specific information about the technique \cite{Brownlee2010a}.
% this report
This report...

A pervasive problem in the field of optimization algorithms is the lack of meaningful and consistent algorithm benchmarking methodology. This includes but is not limited to issues of the selection of problem instances, the selection of algorithm specifications, the algorithm configuration parameters, and interpretation of results. The intention of this paper is to summarize the literature related to benchmarking optimization algorithms, with a focus on benchmarking in the face of the “no free lunch” theorem, and useful statistical tools for interpreting results. This context for this review is biologically inspired optimization algorithms applied to continuous function optimization, although the principles extend beyond these themes.

When it comes to evaluating an optimization algorithm, every researcher has their own thoughts on the way it should be done. Unfortunately, many empirical evaluations of optimization algorithms are performed and reported without addressing basic experimental design considerations. Perhaps before an experimental methodology can be adopted, a researcher/practitioner may be paralyzed by the perceived pessimism of the no free lunch theorem that contends the futility of the benchmarking exercise. This work will provide a summary of the literature on experimental design and empirical algorithm comparison methodology. This summary will contain rules of thumb and perhaps the seeds of best practice when attempting to configure and compare optimization algorithms, specifically in the face of the no free lunch theorem.

% 
% Issues of Benchmarking Methodology
% 
\section{Issues of Benchmarking Methodology}
Empirically	comparing	the	performance	of algorithms on optimization problem instances is a staple for the fields of heuristics and biologically inspired computation, and the problems of effective comparison methodology have been discussed since the inception of these fields. Johnson is an excellent place to start suggesting that the coding of an algorithm is the easy part of the process, that the difficult work is getting meaningful and publishable results [12]. He goes on to provide a very through list of questions to consider before racing algorithms, as well as what he describes as his pet peeves within the field of empirical algorithm research.

Hooker [24] (among others) practically condemns what he refers to as competitive testing of heuristic algorithms, calling it fundamentally anti-intellectual . He goes on to strongly encouraging a rigorous methodology of what he refers to as scientific testing where the aim is to investigate algorithmic behaviors. Barr, Golden, et al. [43] list a number of properties worthy of a heuristic method making a contribution, which can be paraphrased as; efficiency, efficacy, robustness, complexity, impact, generalizability and innovation. This is interesting given that many (perhaps a majority) of conference papers focus on solution quality alone (an aspect of efficacy).

Barr, Golden et al. [43] in their classical work on reporting empirical results of heuristics specify a loose experimental setup methodology with the following steps; 1) define the goals of the experiment, 2) select measure of performance and factors to explore, 3) design and execute the experiment, 4) Analyze the data and draw conclusions, and finally 5) report the experimental results. They then suggest eight guidelines for reporting results, in summary they are; reproducibility, specify all influential factors (code, computing environment, etc), be precise regarding measures, specify parameters, use statistical experimental design, compare with other methods, reduce variability of results, ensure results are comprehensive. They then go on to clarify these points with examples.

Peer, Engelbrecht et al. [17] summarize the problems of algorithm benchmarking (with a bias toward particle swarm optimization) to the following points; duplication of effort, insufficient testing, failure to test against state- of-the-art, poor choice of parameters, conflicting results, invalid statistical inference. Eiben and Jelasity [2] site four problems with the state of benchmarking evolutionary algorithms; 1) test instances are chosen ad hoc from the literature, 2) results are provided without regard to research objectives, 3) scope of generalized performance is generally too broad, 4) results are hard to reproduce.
Gent and Walsh provide a summary of simple dos and don ts for experimentally analyzing algorithms [22]. For an excellent introduction to empirical research and experimental design in artificial intelligence see Cohen [42].

The theme of the classical works on algorithm testing methodology [2,24,43,44] is that there is a lack of rigor in the field. This section will discuss three main problem areas to consider before benchmarking, namely 1) treating algorithms as complex systems that need to be tuned before applied, 2) considerations when selecting problem instances for benchmarking, and 3) the selection of measures of performance and statistical procedures for testing experimental hypotheses. A final section 4) covers additional best practice to consider.


% 
% Selecting Algorithm Parameters
% 
\section{Selecting Algorithm Parameters}
Optimizations algorithms are parameterized, although in the majority of cases the affect of adjusting algorithm parameters is not fully understood. This is because unknown non-linear dependencies commonly exist between the variables resulting in the algorithm being consider a complex system. Further, the NFLT warns us to be careful generalizing the performance of parameters across problem instances, problem classes, and domains. Finally, given that algorithm parameters are typically a mixture of real and integer numbers exhaustively enumerating the parameter space of an algorithm is infeasible.

There are many solutions to this problem such as self-adaptive parameters, meta-algorithms for searching for good parameters values and methods of performing sensitivity analysis over parameter ranges. A good introduction to the parameterization of genetic algorithms is Lobo, Lima, et al. [20]. The best and self- evident place to start (although often ignored [2]) is to investigate the literature and see what parameters been used historically. Although not a robust solution, it may prove to be a useful starting point for further investigation. The traditional approach is to run an algorithm on a large number of test instances and generalize the results [23]. We haven t really come much further than this historical methodology other than perhaps the application of more and differing statistical methods to decrease effort and better support findings.

As mentioned, an area of study involves treating the algorithm as a complex systems where problem instances may become yet another parameter of the model [3,19]. From here, sensitivity analysis can be performed in conjunction with statistical methods to discover parameters that have the greatest effect [29] and perhaps generalize model behaviors.

Francois and Lavergne [41] mention the deficiencies of the traditional trial-and-error and experienced- practitioner approaches to parameter tuning, further suggesting	that	seeking	general	rules	for parameterization will lead to optimization algorithms that offer neither convergent or efficient behaviors. They offer a statistical model for evolutionary algorithms that describes a functional relationship between algorithm parameters and performance. Nannen and Eiben [54,55] propose a statistical approach called Calibration and Relevance Estimation (CRE) to estimating the relevance of parameters in a genetic algorithm. Coy, Golden, et al. [48] use a statistical steepest decent method procedure for locating good parameters for metaheuristics on many different combinatorial problem instances.

Bartz-Beielstein [51] use a statistical experimental design methodology to investigate the parameterization of the Evolutionary Strategy (ES) algorithm. A sequential statistical methodology is proposed by Bartz- Beielstein, Parsopoulos, et al. [49] for investigating the parameterization, and comparison between the Particle Swarm Optimization (PSO) algorithm, the Nelder-Mead Simplex Algorithm (direct search), and the Quasi- Newton algorithm (derivative-based). Finally, an approach that is popular within the metaheuristic and Ant Colony Optimization (ACO) community is to use automated Monte Carlo and statistical procedures for sampling discretised parameter space of algorithms on benchmark problem instances [38] (software available [37]). Similar racing procedures have also been applied to evolutionary algorithms [4].

% 
% Problem Instances
% 
\section{Problem Instances}
This section focuses on issues related to the selection of function optimization test instances, but the general theme of cautiously selecting problem instances is clearly generally applicable.

Common lists of test instances include; De Jong [30], Fogel [14], and Schwefel [21]. Yao, Lui, et al. [57] list many canonical test instances as does Schaffer, Caruana, et al. [23]. Moving beyond static instances, Spears and Potter [56] maintain a list of function generators, some with source code. Gallagher and Yuan [34] review test function generators and propose a tunable mixture of Gaussians test problem generator. Finally, McNish [6] propose using fractal based test problem generators via a standardized web interface (available [7]).

The division of test problems into classes is another axiom of modern optimization algorithm research, although the issues with this methodology are the taxonomic criterion for problem classes and on the selection of problem instances for classes.

Eiben and Jelasity [2] strongly support the division of problem instances into categories and encourage the evaluation of optimization algorithm over a large number of test instances. They suggest classes could be natural (taken from the real world), or artificial (simplified or generated). In their paper on understanding the interactions of GA parameters Deb and Agrawal [28] propose four structural properties of problems for testing genetic algorithms; multi-modality, deception, isolation, and collateral noise. Yao, Lui, et al. [57] divide their large test dataset into the categories of unimodal,	multimodal-many	local	optima,	and multimodal-few local optima. Whitley, Rana, et al. [13] provide a detailed study on the problems of selecting test instances for genetic algorithms. They suggest that difficult problem instances should be nonlinear, non- separable, and non-symmetric.

English [52] suggests that many functions in the field of EC are selected based on structures in the response surface (as demonstrated in the above examples), and that they inherently contain a strong Euclidean bias. The implication in the context of NFLT of course is that the algorithms already have some a priori knowledge about the domain built into them and that results are reported on an already restricted problem set. This is a reminder that instance are selected to demonstrate algorithmic behavior on a narrow domain type.

% 
% Measure and Statistical Methods
% 
\section{Measure and Statistical Methods}
There are many ways to measure the performance of an optimization algorithm for a problem instance, although the most common involves a quality (efficacy) measure of solution(s) found (see the following for lists and discussion of common performance measures [2,18,32,43,49]). Most biologically inspired optimization algorithms have a stochastic element, typically in their random starting position(s) and in the probabilistic decisions made during sampling of the domain. Thus the measuring of performance must be repeated a number of times3 to account for the stochastic variance, which also could be a measure of comparison between algorithms.

Ultimately, irrespective of the measures used, sound statistical experimental design requires the specification of 1) a null hypothesis (no change), 2) alternative hypotheses (difference, directional difference), and 2) acceptance or rejection criteria for the hypothesis. In a typically	case	of	comparing	stochastic-based optimization algorithms on a problem instance; the null hypothesis is commonly stated as the equality between two or more central tendencies (mean or medians) of a quality measure.

Peer, Engelbrech, et al. [17] and Birattari and Dorigo [32] provide a basic introduction (suitable for an algorithm-practitioner) into the appropriateness of various statistical tests for algorithm comparisons. For a good introduction to statistics and data analysis see Peck Olson, Devore [45], for an introduction to non- parametric methods see Holander and Wolfe [39], and for an excellent and detailed presentation of parametric and nonparametric methods and their suitability of application see Sheskin [16]. For an excellent open source software package for performing statistical analysis on data see the R Project [1].

To summarize, parametric statistical methods are used for interval and ratio data (like a real-valued performance measure), and nonparametric methods are used for ordinal, categorical and rank-based data. Interval data is typically converted to ordinal data when salient constraints of desired parametric tests (such as assumed normality of distribution) are broken such that the less powerful nonparametric tests can be used. The use of nonparametric statistical tests maybe preferred as some authors [17,33] claim the distribution of cost values are very asymmetric and or not normal. Although it is important to remember that most parametric tests degrade gracefully.

Chiarandini, Basso, et al. [33] provide an excellent case study for using the permutation test (a nonparametric statistical method) to compare stochastic optimizers by running each algorithm once per problem instance, and multiple times per problem instance. While rigorous, their method appears quite complex and their results are difficult to interpret.

Barrett, Marathe, et al. [8] provide a rigorous example of applying the parametric test Analysis of Variance (ANOVA) of three different heuristic methods on a small sample of scenarios. Reeves and Write [9,10] also provide an example of using ANOVA in their investigation into Epistatsis on genetic algorithms. In their tutorial on the experimental investigation of heuristic methods, Rardin and Uzsoy [44] warn against the use of statistical methods, claiming their rigidity as a problem, and the meaningfulness of practical significance over that of statistical significance . They go on in the face of their objections to provide an example of using ANOVA to analyze the results of an illustrative case study.

Finally Peer, Engelbrech, et al. [17] highlights a number of case study example papers that use statistical methods inappropriately. In their OptiBench method, algorithm results are standardized and ranked according to three criteria then compared using the Wilcoxon rank sum test.

% 
% Other
% 
\section{Other}
Another pervasive problem in the field of
optimization is the reproducibility (implementation) of an algorithm. An excellent solution to this problem is making source code available by creating or collaborating with open-source software projects. This behavior may result in implementation standardization, a reduction in the duplication of effort for experimentation and repeatability, and perhaps more experimental accountability [2,17].

Peer, Engelbrech, et al. [17] stress the need to compare to the state-of-the-art implementations rather than the historic canonical implementations to give a fair and meaningful evaluation of performance.

Another area that is often neglected is that of algorithm descriptions, particularly in regard to reproducibility. Pseudocode is often used, although (in most cases) in an inconsistent manner and almost always without reference to a recognized pseudocode standard or mathematical notation such as [25]. Many examples are a mix of programming languages, English descriptions and mathematical notation, making them difficult to follow, and commonly impossible to implement in software due to incompleteness and ambiguity.

Finally, an excellent tool for comparing optimization algorithms in terms of their asymptotic behavior from the field of computation complexity is the Big O notation [5]. In addition to clarifying aspects of the algorithm, it provides a problem independent way of characterizing an algorithms space and or time complexity.

It is clear that there is no silver bullet to experimental design for empirically evaluating and comparing optimization algorithms, rather there are as many methods and options as there are publications on the topic. The field of optimization as of yet has not agreed upon general method of application like the field of data mining (processes such as Knowledge Discovery in Databases (KDD) [53] and CRISP-DM [50]). Although not experimental methods for comparing machine learning algorithms, these processes provide a general model to encourage the practitioner to consider important issues before application of an approach.

Finally, it is worth pointing out a paper by De Jong [31] (somewhat controversially titled) that provides a reminder that although the genetic algorithm has been shown to solve function optimization, it is not necessarily innately a function optimizer, and rather that function optimization is a demonstration of the complex adaptive systems ability to learn. It is a reminder to be careful not to link an approach too tightly with a domain, particularly if the domain was chosen for demonstration purposes.

% summarise the document message and areas for future consideration
\section{Conclusions}
\label{sec:conclusions}
This report...

The content for this report was based on a prior technical report by the author on benchmarking methodology \cite{Brownlee2007f}.

% bibliography
\bibliographystyle{plain}
\bibliography{../bibtex}

\end{document}
% EOF